defaults:

  # Trainer
  logdir: /dev/null
  run: train_fixed_eval
  seed: 0
  task: dummy_discrete
  env: {amount: 1, parallel: none, daemon: False, repeat: 1, size: [64, 64], camera: -1, gray: False, length: 0, discretize: 0, lives: False, sticky: True, restart: True, seed: 0}
  replay: fixed
  replay_size: 1e6
  replay_fixed: {length: 64, prio_starts: 0.0, prio_ends: 1.0, sync: 0}
  replay_consec: {length: 64, sync: 0}
  replay_prio: {length: 64, prio_starts: 0.0, prio_ends: 1.0, sync: 0, fraction: 0.1, softmax: False, temp: 1.0, constant: 0.0, exponent: 0.5}
  tf: {jit: True, platform: gpu, precision: 16}
  eval_dir: ''
  filter: '.*'
  tbtt: 0
  train:
    steps: 1e8
    expl_until: 0
    log_every: 1e4
    eval_every: 3e4
    eval_eps: 1
    eval_samples: 1
    train_every: 4
    train_steps: 1
    train_fill: 1e4
    eval_fill: 1e4
    pretrain: 1
    log_zeros: False
    log_keys_video: [image]
    log_keys_sum: '^$'
    log_keys_mean: '^$'
    log_keys_max: '^$'
    log_timings: True
    sync_every: 180

  # Agent
  batch_size: 16
  clip_rewards: tanh
  expl_behavior: greedy
  expl_noise: 0.0
  eval_noise: 0.0
  eval_state_mean: False
  data_loaders: 16

  # World Model
  grad_heads: [decoder, reward, discount]
  pred_discount: True
  rssm: {ensemble: 1, hidden: 1024, deter: 1024, stoch: 32, discrete: 32, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
  encoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: none, cnn_depth: 48, cnn_kernels: [4, 4, 4, 4], mlp_layers: 4, mlp_units: 400}
  decoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: none, cnn_depth: 48, cnn_kernels: [5, 5, 6, 6], mlp_layers: 4, mlp_units: 400}
  reward_head: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
  discount_head: {layers: 4, units: 400, act: elu, norm: none, dist: binary}
  loss_scales: {kl: 1.0, reward: 1.0, discount: 1.0}
  kl: {free: 0.0, forward: False, balance: 0.8, free_avg: True}
  model_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100.0, wd: 1e-6}

  # Actor Critic
  actor: {layers: 4, units: 400, act: elu, norm: none, dist: auto, min_std: 0.1}
  critic: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
  actor_opt: {opt: adam, lr: 8e-5, eps: 1e-5, clip: 100.0, wd: 1e-6}
  critic_opt: {opt: adam, lr: 2e-4, eps: 1e-5, clip: 100.0, wd: 1e-6}
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 15
  imag_unroll: True
  actor_grad: auto
  actor_grad_mix: 0.1
  actor_ent: 2e-3
  slow_target: True
  slow_target_update: 100
  slow_target_fraction: 1.0
  slow_baseline: True
  reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}

  # Exploration
  expl_intr_scale: 1.0
  expl_extr_scale: 0.0
  expl_opt: {opt: adam, lr: 3e-4, eps: 1e-5, clip: 100.0, wd: 1e-6}
  expl_head: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
  expl_reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}
  disag_target: stoch
  disag_log: False
  disag_models: 10
  disag_offset: 1
  disag_action_cond: True
  expl_model_loss: kl

nonepisodic: &nonepisodic

  pred_discount: False
  grad_heads: [decoder, reward]

atari:

  task: atari_pong
  env: {gray: True, repeat: 4}
  train.steps: 5e7
  train.eval_every: 2.5e5
  train.train_fill: 50000
  train.train_every: 16
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  clip_rewards: tanh
  rssm: {hidden: 600, deter: 600}
  model_opt.lr: 2e-4
  actor_opt.lr: 4e-5
  critic_opt.lr: 1e-4
  actor_ent: 1e-3
  discount: 0.999
  loss_scales.kl: 0.1
  loss_scales.discount: 5.0

minecraft:

  task: minecraft_diamond
  train.log_keys_max: '^log_inventory.*'
    # env: {amount: 8, parallel: thread}
  env: {amount: 16, parallel: thread}
  tf.precision: 32  # Only 12% slower than fp16 on A100.
  # eval_dir: /gcs/xcloud-shared/danijar/data/2022-01-23-diamond-dataset
  # train.eval_fill: 0
  train.eval_fill: 3e4
  run: train_fixed_eval
  train.eval_samples: 32
  train.train_every: 16
  # warm_start_dir: /gcs/xcloud-shared/danijar/data/2021-12-28-minerl-warmstart
  # train.train_fill: 1e6
  # replay_type: prio

dmlab:

  task: dmlab_rooms_collect_good_objects_train
  encoder: {mlp_keys: 'reward', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  env.repeat: 4
  train.steps: 5e7
  env: {amount: 8, parallel: process}
  train.train_every: 16
  loss_scales.kl: 3.0

crafter:

  task: crafter_reward
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  train.log_keys_max: '^log_achievement_.*'
  train.log_keys_sum: '^log_reward$'
  discount: 0.999
  .*\.norm: layer

dmc_vision:

  <<: *nonepisodic
  task: dmc_walker_walk
  env.repeat: 2
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  train.eval_every: 1e4
  train.train_fill: 1000
  train.pretrain: 100
  clip_rewards: identity
  rssm: {hidden: 200, deter: 200}
  model_opt.lr: 3e-4
  actor_opt.lr: 8e-5
  critic_opt.lr: 8e-5
  actor_ent: 1e-4
  kl.free: 1.0

dmc_proprio:

  <<: *nonepisodic
  task: dmc_walker_walk
  env.repeat: 2
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}
  train.eval_every: 1e4
  train.train_fill: 1000
  train.pretrain: 100
  clip_rewards: identity
  rssm: {hidden: 200, deter: 200}
  model_opt.lr: 3e-4
  actor_opt.lr: 8e-5
  critic_opt.lr: 8e-5
  actor_ent: 1e-4
  kl.free: 1.0

debug:

  env.length: 100
  train:
    eval_every: 300
    log_every: 300
    train_fill: 100
    train_steps: 1
    train_every: 30
  batch_size: 4
  replay_size: 500
  replay_.*\.length: 12
  encoder.cnn_depth: 16
  decoder.cnn_depth: 16
  rssm: {deter: 64, hidden: 64, stoch: 8, discrete: 8}
  .*\.layers: 2
  .*\.units: 64
  tf.jit: False
